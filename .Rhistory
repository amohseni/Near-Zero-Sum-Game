}
# Conduct experiment function
Experiment <- function(HypothesisToTest) {
if (HypothesisToTest == 1) {
result <-
Atrue() # If the optimal hypothesis is chosen, drawn from the corresponding R.V.
} else {
result <-
Aother() # If another hypothesis is chosen, drawn from the other R.V.
}
return(result)
}
### LEARNING PROCESS
# Vector of Dir(r_1,r_2,...,r_n) beliefs about each theory j=1,2,...,n at a given time t
TheoriesBeliefVector <- TheoriesPriors
# Matrix in which to store the history of beliefs about each theory
TheoriesBeliefMatrix <-
matrix(
data = NA,
nrow = Duration + 1,
ncol = length(TheoriesBeliefVector),
byrow = TRUE
)
# Populate the first row of the theories belief matrix with the prior beliefs
TheoriesBeliefMatrix[1,] <- TheoriesBeliefVector
HypothesesBeliefMatrix <- HypothesesPriors
for (round in 1:Duration) {
# Matrix of Beta(p_i,q_i) beliefs about each hypothesis i=1,2,...,n at a given time t
HypothesesBeliefMatrix[, 1] <- TheoriesBeliefVector
HypothesesBeliefMatrix[, 2] <- sum(TheoriesBeliefVector)
# Initiate relevant variables
HistoryHypothesis <- c() # Vector of hypotheses tested
HistoryResult <- c() # Vector of experimental results obtained
Converged <-
FALSE # Status of learning, stops on having converged to a hypothesis
t <- 0 # Time counter
# Continue experimenting with hypotheses until having converged
while (Converged == FALSE) {
# Increment time
t <- t + 1
### HYPOTHESIS TEST
# Choose a hypothesis to test
Hypothesis <- BestResponse()
# Conduct an experiment of that hypothesis
Result <- Experiment(Hypothesis)
# Update beliefs after an experiment
if (Result == 1) {
HypothesesBeliefMatrix[Hypothesis, 1] <-
HypothesesBeliefMatrix[Hypothesis, 1] + 1 # If the result was a success, then update the p parameter of the Beta(p,q) posterior of the hypothesis tested
} else {
HypothesesBeliefMatrix[Hypothesis, 2] <-
HypothesesBeliefMatrix[Hypothesis, 2] + 1 # If the result was a failure, then update the q parameter of the Beta(p,q) posterior of the hypothesis tested
}
# Print matrix of current beliefs
print(HypothesesBeliefMatrix)
# Store hypothesis and results in the history
HistoryHypothesis <-
append(HistoryHypothesis, Hypothesis, after = length(HistoryHypothesis))
HistoryResult <-
append(HistoryResult, Result, after = length(HistoryResult))
# Determine whether learning has converged
if (t > 30) {
# If the process has repeated enought times, we takes this to indicate convergence
RecentHistory <- HistoryHypothesis[(t - 20):t]
if (length(unique(RecentHistory)) == 1 | t == 1000) {
Converged <- TRUE
# If we have CONVERGED, then compute final probabilies
HypothesesFinalBeliefMatrix <- HypothesesBeliefMatrix
for (i in 1:n) {
rowTotal <- sum(HypothesesFinalBeliefMatrix[i, ])
HypothesesFinalBeliefMatrix[i, ] <-
HypothesesFinalBeliefMatrix[i, ] / rowTotal
}
FinalHypothesis <- which.max(HypothesesFinalBeliefMatrix[, 1])
# UPDATE BELIEF over the higher-level THEORIES
TheoriesBeliefVector[FinalHypothesis] <- TheoriesBeliefVector[FinalHypothesis] + 1
# Given that learning has converged to hypothesis H_k,
# update the belief over theories Dir(p_2, ..., p_n)
# by incrementing the value of the coressponding theory parameter p_k.
# Save the current belief over the higher-level theories in the belief matrix
TheoriesBeliefMatrix[round + 1, ] <- TheoriesBeliefVector
# Print out the HISTORY of LEARNING
# consisting of hypotheses tested and results obtained
print("History of hypotheses tested:")
print(HistoryHypothesis)
print("History of results obtained:")
print(HistoryResult)
# Print out the FINAL HYPOTHESIS, and POSTERIOR BELIEFS
print("Posterior probabilites on hypotheses:")
print(HypothesesBeliefMatrix)
print(paste(
"Learning has converged to: HYPOTHESIS ",
FinalHypothesis,
sep = ""
))
}
}
}
# Compuste the final belief vector.
TheoriesFinalBeliefMatrix <-
matrix(
data = NA,
nrow = Duration + 1,
ncol = TheoriesNum,
byrow = TRUE
)
for (i in 1:(Duration + 1)) {
TheoriesFinalBeliefMatrix[i,] <-
TheoriesBeliefMatrix[i,] / sum(TheoriesBeliefMatrix[i,])
}
# Print the evolution of the belief in the theories
print("History of posterior proabilities on theories:")
print(TheoriesFinalBeliefMatrix)
# Determine the final theory
FinalBeliefVector <- TheoriesFinalBeliefMatrix[nrow(TheoriesFinalBeliefMatrix),]
FinalBelief <- which.max(FinalBeliefVector)
# Print the final theory
print(paste("Learning has converged to: THEORY ",
FinalBelief,
sep = ""))
}
# Plot results
df <- data.frame(c(0:Duration), TheoriesFinalBeliefMatrix)
dfMelt <- melt(df, id.vars = 1)
colnames(dfMelt) <- c("Time", "Theory", "Belief")
ggplot(data = dfMelt, aes(x = Time, y = Belief)) +
geom_line(aes(group = Theory, color = Theory), size = 1)
### EOD
shiny::runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
# RATIONALITY AND PREDICTION IN
# NEAR-ZERO-SUM GAMES OF UNCERTAIN INFORMATION
# << UI >>
# by Aydin Mohseni
# Load the shiny GUI library
library(shiny)
library(ggplot2)
library(ggthemes)
# Set encoding for special characters
Sys.setlocale("LC_ALL", "fr_FR.UTF-8")
# Define UI for application
shinyUI(fluidPage(
# CSS for visual
includeCSS("www/style.css"),
# Title
titlePanel("Rationality and Prediction in Near-Zero-Sum Games of Uncertain Information"),
# Load MathJax
withMathJax(),
fluidRow(
style = "background-color:#F2F2F2; margin-top: 30px; margin-bottom: 30px; padding: 10px",
column(
width = 4,
# Introduction text:
p(
tags$b("Premise:"),
"The Nash equilibrium concept has come to be central to game theory, economics, and other social sciences. A perennial open question is the extent and condition under which we can expect rational agents to play according to the Nash equilibria of games."
)
),
column(
width = 4,
# Introduction text:
p(
"In ''The Impossibility of Predicting the Behavior of Rational Agents'' (2001) Foster & Young provide an impossibility theorem demonstrating that, in near-zero-sum of uncertain information, agents cannot be both rational and also accurately predict the strategies of other agents, and so cannot play the Nash equilibria of such games."
)
),
column(
width = 4,
p(
tags$b("Result:"),
"In contrast to Foster & Young's result, this model demonstrates how, when rational agents are less-than-perfect in their implementations their plans, they can converge (in the limit) to accurate prediction, and once again learn to play Nash."
)
)
),
# Sidebar for Parameter Input
sidebarLayout(
sidebarPanel(
# selectInput("interaction_structure", "Game Type:",
#     c("Neutral Game", "Dominating Strategy Game", "Coordination Game", "Anti-Coordination Game", "Custom"),
#     selected = "Custom"
#   ),
# 2X2 Game Payoff Input Fields
strong("Payoff Matrix:"),
# 1st Row: Types
fluidRow(
column(4),
column(4,
p("Type A", align="center", style="margin-top:7px")
),
column(4,
p("Type B", align="center", style="margin-top:7px")
)),
# 2nd Row: Payoffs to Type A
fluidRow(
column(4,
p("Type A", align="right", style="margin-top:7px;")
),
column(4,
numericInput("a",
label = NULL,
value = 1,
min = 1)
),
column(4,
numericInput("b",
label = NULL,
value = 3,
min = 1)
)),
# 3rd Row: Payoffs to Type B
fluidRow(
column(4,
p("Type B", align="right", style="margin-top:7px;")
),
column(4,
numericInput("c",
label = NULL,
value = 2,
min = 1)
),
column(4,
numericInput("d",
label = NULL,
value = 1,
min = 1)
)),
# Population Size Slider
sliderInput("populationSize",
"Population Size:",
min = 2,
max = 200,
value = 80),
# Mutation Rate Slider
sliderInput("mutationRate",
"Mutation Rate:",
min = 0,
max = .1,
value = .02),
# Intensity of Selection Slider
sliderInput("intensityOfSelection",
"Intensity of Selection:",
min = 0,
max = 1,
value = .25),
# Simulate Single Population Button
tags$head(tags$script(src = "message-handler.js")),
p(actionButton("simulateSinglePopulation", "RUN EVOLUTION"), align = "center"),
# Simulation Time Slider
sliderInput("simulationTime",
"Simulation Time:",
min = 1,
max = 10000,
value = 4000)
),
# Main Panel with Stationary Distribution + Simulation & Stats Panels
mainPanel(
# plotOutput("stationaryDistribution")
)
)
))
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
# Prediction & Rationality
# Response to Foster & Young (2000)
# 2x2 Zero Sum Game
# Load packages from library
library(ggplot2)
library(ggthemes)
# Set benchmark game payoffs
a = b = c = d = 1
# for the 2x2 zero sum game
#         H       T
#   H | a, -a | b, -b |
#   T | c, -c | d, -d |
# Assign benchmark game payoffs for both players
p1Payoff0 <- data.frame(c(a,-b), c(-c, d))
rownames(p1Payoff0) <- c("H", "T")
colnames(p1Payoff0) <- c("H", "T")
p2Payoff0 <- data.frame(c(-a, b), c(c,-d))
rownames(p2Payoff0) <- c("H", "T")
colnames(p2Payoff0) <- c("H", "T")
# v-Perturbation of payoffs
vPertub <- function() {
z <- rbeta(1, 1, 1) - (1 / 2)
return(z)
}
# Perturb payoffs
for (i in 1:2) {
for (j in 1:2) {
p1Payoff0[i,j] <- p1Payoff0[i,j] + .1 * vPertub()
p2Payoff0[i,j] <- p2Payoff0[i,j] + .1 * vPertub()
}
}
# Expected payoff functions
expPayoff <- function(p, y) {
# Use the appropriate payoff table for the player
if (y == 1) {
w <- p1Payoff0
} else {
if (y == 2) {
w <- p2Payoff0
} else {
print("expPayoff: input not a player")
}
}
# Create empty payoffs
z <- c(NA, NA)
# Calculate expected payoff for playing Heads
z[1] <- p * w[1, 1] + (1 - p) * w[1, 2]
# Calculate expected payoff for playing Tails
z[2] <- p * w[2, 1] + (1 - p) * (w[2, 2])
# Return the expected payoffs for each H and T
return(z)
}
# # Learning process: Empirical best response
# BestResponse <- function(x, y) {
#   # For the first round of play, choose a random action
#   if (x == 1) {
#     z <- c(ifelse(rbinom(1, 1, .5) == 1, "H", "T"), 0)
#     return(z)
#   }
#   if (x > 1) {
#     # For all other rounds of play, calculate the best response for the given player
#     # to the empirical distribution of the other player's past play
#     hTable <- table(factor(h[, (3 - y)], levels = c("H", "T")))
#     freqOfHeads <- as.numeric(hTable[1] / sum(hTable))
#     payoffs <- expPayoff(freqOfHeads, y)
#     if (payoffs[1] > payoffs[2]) {
#       # If the best response is heads, play heads
#       return(c("H", freqOfHeads))
#     } else {
#       if (payoffs[1] < payoffs[2]) {
#         # If the best response is tails, play tails
#         return(c("T", freqOfHeads))
#       } else {
#         # Otherwise, randomly choose a strategy
#         return(c(ifelse(rbinom(1, 1, .5) == 1, "H", "T"), freqOfHeads))
#       }
#     }
#   }
# }
LogitBestResponse <- function(x, y) {
# For the first round of play, choose a random action
if (x == 1) {
return(c(ifelse(rbinom(1, 1, .5) == 1, "H", "T"), 0))
}
if (x > 1) {
# For all other rounds of play, calculate the best response for the given player
# to the empirical distribution of the other player's past play
hTable <- table(factor(h[, (3 - y)], levels = c("H", "T")))
freqOfHeads <- as.numeric(hTable[1] / sum(hTable))
payoffs <- expPayoff(freqOfHeads, y)
mixedStrategy <-
exp(λ  * payoffs[1]) / (exp(λ  * payoffs[1]) + exp(λ  * payoffs[2]))
return(c(
ifelse(runif(1, 0, 1) < mixedStrategy, "H", "T"),
mixedStrategy,
freqOfHeads
))
}
}
# Determing number of rounds of play in simulation
T <- 1000
# Create blank history to record play
h <-
data.frame(
P1 = character(T),
P2 = character(T),
P1predictionOfP2 = numeric(T),
P2predictionOfP1 = numeric(T),
P1beliefOfP2 = numeric(T),
P2beliefOfP1 = numeric(T),
stringsAsFactors = FALSE
)
### Simulate game for BR dynamics
# for (i in 1:T) {
#   h[i, 1] <- BestResponse(i, 1)[1]
#   h[i, 2] <- BestResponse(i, 2)[1]
#   h[i, 3] <- round(as.numeric(BestResponse(i, 1)[2]), 4)
#   h[i, 4] <- round(as.numeric(BestResponse(i, 2)[2]), 4)
# }
### Simulate game for Logit dynamics
λ  <- 1 # Set rationality parameter λ
for (i in 1:T) {
LBR1 <- LogitBestResponse(i, 1)
LBR2 <- LogitBestResponse(i, 2)
h[i, 1] <- LBR1[1]
h[i, 2] <- LBR2[1]
h[i, 3] <- round(as.numeric(LBR1[2]), 4)
h[i, 4] <- round(as.numeric(LBR2[2]), 4)
h[i, 5] <- round(as.numeric(LBR2[3]), 4)
h[i, 6] <- round(as.numeric(LBR1[3]), 4)
}
# Output history of play
h[1, c(3:6)] <- 0.5
print(h)
dfStrategy <-
data.frame(c(1:T), c(as.character(rep(1, T)), as.character(rep(2, T))), c(h[, 3], h[, 4]))
colnames(dfStrategy) <- c("Round", "Player", "Strategy")
ggplot(dfStrategy,
aes(
x = Round,
y = Strategy,
group = Player,
colour = Player
)) +
geom_path(alpha = 1, size = 0.8) +
ggtitle("Individual (Logit Choice) Strategy") +
labs(x = "Round Number", y = "Strategy") +
ylim(0:1) +
scale_color_manual(values = c("gray30", "red2")) +
theme_few()
dfBelief <-
data.frame(c(1:T), c(as.character(rep(1, T)), as.character(rep(2, T))), c(h[, 5], h[, 6]))
colnames(dfBelief) <- c("Round", "Player", "Prediction")
ggplot(dfBelief,
aes(
x = Round,
y = Prediction,
group = Player,
colour = Player
)) +
geom_path(alpha = 1, size = 0.8) +
ggtitle("Prediction of Opponent Strategy") +
labs(x = "Round Number", y = "Prediction") +
ylim(0:1) +
scale_color_manual(values = c("gray30", "red2")) +
theme_few()
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
T
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
shiny::runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
h
dfBelief
runApp('GitHub/Near-Zero-Sum-Game')
h
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
shiny::runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
runApp('GitHub/Near-Zero-Sum-Game')
